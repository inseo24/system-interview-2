# 2장 주변친구

일반적인 지도 기반 근접성 서비스와 다른 점은 근접성 서비스의 사업장 위치는 정적이나, 주변 친구의 위치는 동적으로 변한다는 점이다.

## 질문

1. '주변'의 반경이 어느 정도인지? 설정 가능해야 하는지?
2. 나와 주변 친구 사이 거리는 직선 거리인지? 중간에 지형지물이 있어 실제로 더 이동해야 할 수도 있는데 직선거리로 가정해도 되는지?
3. 얼마나 많은 사용자가 이 앱을 이용하고 그 중 몇 %의 사용자가 이 기능을 이용하는지?
4. 사용자의 이동 이력을 보관해야 하는지?
5. 사용자가 10분 이상 비활성 상태일 경우, 목록에서 사라지게 해야하는지? 아니면 마지막 위치를 표시해야 하는지?
6. GDPR(General Data Protection Regulation)이나 CCPA(California Consumer Privacy Act)와 같은 개인정보 보호법을 준수해야 하는지?


## 요구사항

### 기능 요구사항

- 사용자는 모바일 앱에서 주변 친구를 확인할 수 있다. 
- 주변 친구 목록에는 해당 친구까지의 거리, 그리고 해당 경로가 마지막으로 갱신된 시간(timestamp)이 함께 표시되어야 한다.
- 친구 목록은 몇 초마다 한 번씩 갱신되어야 한다.

### 비기능 요구사항

- 낮은 지연 시간(low latency): 주변 친구의 위치 변화가 반영되는 데 너무 오랜 시간이 걸리지 않아야 한다.
- 안전성: 시스템은 전반적으로 안정적이어야 하지만, 때로 몇 개 데이터가 유실되는 것 정도는 용인할 수 있다.
- 결과적 일관성(eventual consistency): 사용자가 주변 친구 목록을 확인할 때, 최신 정보가 아니더라도 괜찮다.


## 개략적 규모 추정

- '주변 친구'는 5마일(약 8km) 이내에 있는 친구로 정의한다.
- 친구 위치 정보는 30초 주기로 갱신, 사람이 걷는 속도가 시간당 4~6km 정도로 느리기 때문이다. 이 속도로 30초 정도 이동한다 해서 주변 친구 검색 결과가 크게 달라지지 않는다.
- 평균적으로 매일 주변 친구 검색 기능을 활용하는 사용자는 1억명으로 가정한다.
- 동접 사용자 수는 DAU 수의 10%로 가정한다. 따라서 동접 사용자 수는 1천만명이다.
- 평균적으로 한 사용자는 400명의 친구를 갖는다고 가정한다. 그리고 그 모두가 주변 친구 검색 기능을 활용한다고 가정한다.
- 페이지당 20명의 주변 친구를 표시하고, 사용자 요청이 있으면 더 많은 주변 친구를 보여준다.

### QPS 계산

- 동시 접속 사용자: 1억 * 10% = 1천만명
- 사용자는 30초마다 자기 위치를 시스템에 전송한다.
  - QPS = 1천만명 / 30초 =~ 334,000

  
## 개략적 설계

- 개념적으로 보면 순수한 P2P 방식으로도 해결 가능하다. 활성 상태인 근방 모든 친구와 지속적인 통신 상태를 유지하면 된다.
  - 모바일 단말은 통신 연결 상태가 좋지 않은 경우도 있고 사용할 수 있는 전략이 충분하지 않아 실용적인 아이디어는 아니지만, 추구해야 할 설계 방향에 대해 아이디어를 얻을 수 있다.
- 좀 더 실용적인 설계안은 공용 백엔드를 사용하는 것이다. 이 백엔드에선 아래 역할을 한다. 
  - 모든 활성 상태 사용자의 위치 변화 내역을 수신
  - 사용자 위치 변경 내역을 수신할 때마다 해당 사용자의 모든 활성 상태 친구를 찾아 그 친구들의 단말로 변경 내역을 전달
  - 두 사용자 사이의 거리가 특정 임계치보다 먼 경우에는 변경 내역을 전송하지 않는다.
- 위 백엔드를 사용하는 경우 큰 규모에 적용하기 쉽지 않다.
  - 천만명의 위치 정보를 30초마다 갱신한다고 하면 초당 334,000번의 위치 정보 갱신을 처리해야 한다.
  - 평균적으로 사용자 1명은 400명의 친구를 갖는다고 하고, 그 가운데 10%가 인근에서 활성화 상태라고 가정하면 초당 334,000 * 400 * 10% = 1400만 건의 위치 정보 갱신 요청을 처리해야 한다.
  - 또한 엄청난 양의 갱신 내역을 사용자 단말로 보내야 한다.

### 주기적 위치 갱신

1. 모바일 클라이언트가 위치가 변경된 사실을 로드밸런서에 전송한다.
2. 로드밸런서는 그 위치 변경 정보를 해당 클라이언트와 웹소켓 서버 사이에 설정된 연결을 통해 웹소켓 서버로 보낸다.
3. 웹소켓 서버는 해당 이벤트를 위치 이동 이력 데이터베이스에 저장한다.
4. 웹소켓 서버는 새 위치를 위치 정보 캐시에 저장하고 TTL도 새롭게 갱신한다. 웹 소켓 서버는 웹소켓 연결 핸들러 안의 변수에 해당 위치를 반영하고, 이 값은 뒤이은 거리 계산 과정에 이용된다.
5. 웹소켓 서버는 레디스 펍/섭 서버의 해당 사용자 채널에 새 위치를 발행한다. 3번에서 5번의 단계는 병렬 처리한다.
6. 레디스 펍/섭 채널에 발행된 새로운 위치 변경 이벤트는 모든 구독자(웹소켓 이벤트 핸들러)에게 브로드캐스트된다. 각 구독자의 웹소켓 연결 핸들러는 친구의 위치 변경 이벤트를 수신하게 된다.
7. 메시지를 받은 웹소켓 서버, 즉 이벤트를 받는 웹소켓 연결 핸들러가 위치한 웹소켓 서버는 새 위치를 보낸 사용자와 메시지를 받은 사용자(웹소켓 연결 핸들러 내의 변수에 그 위치가 보관) 사이 거리를 새로 계산한다.
8. 계산한 거리가 검색 반경을 넘지 않는다면, 새 위치 및 해당 위치로의 이동이 발생한 시각을 나타내는 타임스탬프를 해당 구독자의 클라이언트 앱으로 전송한다. 검색 반경을 넘는 경우에는 보내지 않는다.


- 위 과정은 모든 구독자에게 반복 적용된다. 한 사용자당 평균 400명의 친구가 있으며 그 가운데 10% 가량이 주변에서 온라인 상태일 것으로 가정하였으므로, 한 사용자의 위치가 바뀔 때마다 위치 정보 전송은 40건 정도 발생할 것이다.


### API 설계

#### 웹소켓

- 사용자는 웹소켓 프로토콜을 통해 위치 정보 변경 내역을 전송하고 수신한다. 최소한 다음 API를 제공해야 한다.
  - 서버
    - 주기적인 위치 정보 갱신
        - 요청: 위도, 경도, 시각 정보
        - 응답: 없음
    - 웹소켓 초기화
      - 요청: 클라이언트 위도, 경도, 시각 정보 전송
      - 응답: 자기 친구들의 위치 데이터 수신
  - 클라이언트
    - 갱신된 친구 위치를 수신하는 데 사용할 API
      - 전송되는 데이터: 친구 위치 데이터와 변경된 시각을 나타내는 타임스탬프
    - 새 친구 구독 API
      - 요청: 친구 ID
      - 응답: 가장 최근의 위도, 경도, 시각 정보
    - 구독 해지 API
      - 요청: 친구 ID
      - 응답: 없음

#### HTTP
    
- 친구 추가/삭제, 사용자 정보 갱신 등의 작업 처리

### 데이터 모델

#### 위치 정보 캐시

- 위치 정보 캐시는 사용자 ID를 키로 사용하고, 해당 사용자의 위치 정보(위도, 경도, 시각)를 값으로 사용한다.
- DB를 사용하지 않는 이유
  - '주변 친구' 기능은 사용자의 **현재 위치**만을 기반으로 한다. 따라서 사용자의 위치 정보는 하나만 보관해도 충분해서 영속성을 보장할 필요가 없다.
  - 레디스는 이런 목적에 아주 적합한데, 읽기 및 쓰기 연산 속도가 매우 빠르기 때문이다.
  - TTL도 지원하므로 활성상태가 아닌 사용자 정보를 자동으로 제거할 수도 있다.
  - 영속성을 보장할 필요가 없기 때문에 레디스 서버 하나에 장애가 발생하면 다른 새 서버로 바꾼 다음, 갱신된 위치 정보가 캐시에 채워지게 기다리면 충분하다.


#### 위치 이력 데이터베이스

- 스키마엔 사용자 ID, 위도, 경도, 시각 정보(timestamp)를 포함한다.
- 막대한 쓰기 연산 부하를 감당할 수 있고, 수평적 규모 확장이 가능한 DB가 필요하다. 
- 그런 목적에서 카산드라가 적합하다. RDB를 사용할 수도 있으나 이력 데이터 양이 서버 한 대에 보관하기에는 너무 많을 수 있으므로 샤딩이 필요하다. 
- 사용자 ID를 기준 삼는 샤딩 방안이 가장 기본이다. 부하를 모든 샤드에 고르게 분산시킬 수 있고, 데이터베이스 운영 관리도 간편한 방법이다.

## 상세 설계

### 컴포넌트별 규모 확장성

#### API 서버

- 여기선 다루지 않음
- 따로 생각해보면 좋을듯? RESTful API 서버로 구성된 클러스터 규모를 CPU 사용률이나 부하, IO 상태에 따라 자동으로 늘리는 방법은 다양함

#### 웹소켓 서버

- 웹소켓 클러스터의 경우 stateful이므로 기존 서버를 제거할 때 주의해야 한다.
- 노드를 실제 제거하기 전에 우선 기존 연결부터 종료될 수 있도록 해야 한다. 
- 이를 위해, 로드밸런서가 인식하는 노드 상태를 '연결 종료 중(draining)'으로 변경해야 한다. 이 상태로 변경된 노드는 새로운 웹소켓 연결을 받지 않고, 기존 연결이 종료될 때까지 기다린다.
- 그리고 나서 모든 연결이 종료되면 노드를 제거한다.
- 웹소켓 서버에 새로운 버전의 애플리케이션 소프트웨어를 설치할 때도 마찬가지로 유의해야 한다.
- 즉, 상태가 있는 클러스터의 규모를 자동으로 확장하려면 좋은 로드밸런서가 있어야 한다는 뜻이다. 대부분의 클라우드 로드밸런서에서는 이런 일을 잘 처리한다.


#### 클라이언트 초기화

- 웹소켓 연결이 초기화되면 클라이언트는 해당 모바일 단말의 위치 정보를 전송한다. 그 정보를 받은 웹소켓 연결 핸들러는 다음 작업을 수행한다.

1. 위치 정보 캐시에 보관된 사용자의 위치 갱신
2. 해당 위치 정보는 뒤에서 계산에 이용되므로, 연결 핸들러 내의 변수에 저장함
3. 사용자 DB를 통해 그 사용자의 모든 친구 정보를 가져옴
4. 위치 정보 캐시에 batch 요청을 보내서 모든 친구의 위치를 한번에 가져온다. 
5. 캐시에서 전달받은 친구 위치 각각에 대해 웹소켓 서버에서 해당 친구와 사용자 사이 거리를 계산함. 그 거리가 검색 반경 이내이면 해당 친구의 상세 정보, 위치, 그리고 해당 위치가 마지막으로 확인된 시각 정보를 웹소켓 연결을 통해 클라이언트에 반환
6. 웹소켓 서버는 각 친구의 레디서 서버 펍/섭 채널을 구독함, 채널 생성 및 구독 비용은 저렴하므로 모든 친구 채널을 구독하는데 큰 비용이 없다. 비활성 상태의 친구의 펍/섭 채널을 유지하기 위해 메모리를 사용하긴 하나 소량이고, 활성화 상태로 전환되기 전에는 CPU나 IO를 전혀 이용하지 않는다.
7. 사용자의 현재 위치를 레디스 펍/섭 서버의 전송 채널을 통해 모든 친구에게 전송함


#### 사용자 DB

- 2가지 종류의 데이터가 보관
  - 사용자 정보: 사용자 ID, 이름, 프로필 이미지 URL 등 사용자 상세 정보
  - 친구 관계 데이터
- 규모를 감안하면 데이터를 하나의 관계형 DB로 감당하긴 어렵다. 하지만 사용자 ID를 기준으로 데이터를 샤딩하면 관계형 DB라 하더라도 수평적 규모 확장이 가능하다.


#### 위치 정보 캐시

- TTL값은 해당 사용자 위치 정보가 갱신될 때마다 초기화된다. 따라서 최대 메모리 사용량은 일정 한도 아래로 유지된다.
- 시스템이 가장 바쁜 경우가 천만 명의 사용자가 활성화 상태이며 위치 정보 보관에 100바이트가 필요하다고 가정하면 수 GB 이상의 메모리를 갖춘 최신 레디스 서버 한 대로 모든 위치 정보를 캐시할 수 있다.
  - 하지만 천만 명의 활성 사용자가 대략 30초마다 변경된 위치 정보를 전송한다고 가정하면, 초당 334,000번의 위치 정보 갱신 요청을 처리해야 한다.
  - 이는 조금 부담이 갈 수 있는 수치인데, 캐시할 데이터는 쉽게 샤딩할 수 있다.
  - 각 사용자의 위치 정보는 서로 독립적인 데이터이므로, 사용자 ID 기준으로 여러 서버에 샤딩하면 부하 또한 고르게 분배할 수 있다.
- 가용성을 높이려면 각 샤드에 보관하는 위치 정보를 standby 노드에 복제해 두면 된다. primary 노드에 장애가 발생하면 standby 노드를 신혹하게 primary로 승격시켜 장애시간을 줄인다.


#### 레디스 펍/섭 서버

- 현 설계에서 펍/섭 서버는 위치 변경 내역 메시지의 라우팅 계층으로 활용한다.
- 채널 생성이나 삭제에 비용은 거의 없다. 
- 채널 하나를 유지하기 위해 구독자 관계를 추적하는 목적으로 해시 테이블과 연결 리스트가 필요한데, 아주 소량의 메모리만을 사용한다.
- 오프라인 사용자라 어떤 변경도 없는 채널의 경우에는 생성된 이후에 CPU 자원은 전혀 사용하지 않는다.

1. '주변 친구' 기능을 활용하는 모든 사용자에 채널 하나씩을 부여, 해당 기능을 사용하는 사용자의 앱은 초기화 시 모든 친구의 채널과 구독 관계를 설정함(친구 상태는 고려하지 않음)
2. 한가지 유의할 점은 더 많은 메모리를 사용하게 된다는 점인데, 메모리가 병목이 될 가능성은 낮다. '주변 친구' 기능의 경우, 아키텍처를 단순하게 만들 수 있다면 더 많은 메모리를 투입할 만한 가치는 충분하다.


#### 얼마나 많은 레디스 펍/섭 서버가 필요할까?

#### 메모리 사용량

- 모든 사용자에게 채널 하나씩 할당한다고 하면 필요한 채널의 수는 1억 개다.(10억 사용자의 10%)
- 한 사용자의 활성화 상태 친구 가운데 100명이 주변 친구 기능을 사용한다고 가정하고,
  - 구독자 한 명을 추적하기 위해 내부 해시 테이블과 연결 리스트에 20바이트 상당의 포인터들을 저장해야 한다고 하자.
  - 모든 채널을 저장하는 데는 200GB((1억 * 100명 * 20바이트) / 10^9)가 필요하다.
  - 100GB 메모리를 설치할 수 있는 서버를 사용한다면, 모든 채널을 보관하는 데 레디스 펍/섭 서버는 2대면 된다.

#### CPU 사용량

- 레디스 펍/섭 서버가 구독자에게 전송해야 하는 위치 정보 업데이트 양은 초당 1400만 건에 달한다.
- 보수적으로 기가비트 네트워크 카드를 탑재한 현대적 아키텍처 서버 한 대로 감당 가능한 구독자 수를 100,000라고 가정하자.
- 이 추정치에 따라 필요한 레디스 서버의 수는 1400만/100,000 = 140대 정도이다.
- 위 숫자는 매우 보수적인 수치로 실제 필요한 서버의 수는 훨씬 적을 것이다.

- 위 계산 결과를 통해 다음과 같은 결론을 내릴 수 있다.
  - 레디스 펍/섭 서버의 병목은 메모리가 아니라 CPU 사용량이다.
  - 위 설계가 풀어야 하는 문제의 규모를 감당하려면 분산 레디스 펍/섭 클러스터가 필요하다.


#### 분산 레디스 펍/섭 서버 클러스터

- 모든 채널이 독립적이므로, 메시지를 발행할 사용자 ID를 기준으로 펍/섭 서버들을 샤딩하면 된다.
- service discovery(서비스 탐색) 컴포넌트를 도입해 수백대의 펍/섭 서버 운영을 해보자.
  - etcd, ZooKeeper 등이 가장 널리 사용된다.
  - 이 설계에선 다음 2가지 기능만 사용한다.
    - 가용한 서버 목록을 유지하는 기능 및 해당 목록을 갱신하는 데 필요한 UI나 API
      - 서비스 탐색 컴포넌트는 configuration data를 보관하기 위한 소규모 키-값 저장소라고 보면된다. 
      - ex) 해시 링: 레디스 펍/섭 서버는 메시지를 발행할 채널이나 구독할 채널을 정해야 할 때 이 해시링을 참조한다.
    - 클라이언트(웹소켓 서버)가 '값'에 명시된 레디스 펍/섭 서버에서 발생한 변경 내역을 구독할 수 있도록 하는 기능
      - 웹소켓 서버는 해시링을 참조해 메시지를 발행한 레디스 펍/섭 서버를 선정함
        - 정확한 정보는 서비스 탐색 컴포넌트에 보관되어 있으나 성능 효율을 높이고 싶으면 해시 링 사본을 웹소켓 서버에 캐시하는 것도 가능하다. 단, 사본의 상태를 항상 원본과 동일하게 유지하도록 해야 한다.
      - 웹소켓 서버는 해당 서버가 관리하는 사용자 채널에 위치 정보 변경 내역을 발행


#### 레디스 펍/섭 서버 클러스터의 규모 확장 고려사항

1. 펍/섭 채널에 전송되는 메시지는 메모리나 디스크에 지속적으로 보관되지 않음
   - 채널의 모든 구독자에게 전송되고 나면 바로 삭제
   - 구독자가 아예 없는 경우, 그냥 지워짐
   - 이런 관점에서 펍/섭 채널을 통해 처리되는 데이터는 무상태라고 할 수 있음
2. 펍/섭 서버는 채널에 대한 상태 정보를 보관한다.
   - 각 채널의 구독자 목록은 그 상태 정보의 핵심적인 부분이다.
   - 특정한 채널을 담당하던 펍/섭 서버를 교체하거나 해시 링에서 제거하는 경우 채널은 다른 서버로 이동시켜야 하고, 해당 채널의 모든 구독자에게 그 사실을 알려야 한다.
   - 그런 관점에서 펍/섭은 상태를 가지는 서버다.


- 레디스 펍/섭 서버 클러스터는 상태가 있는 서버 클러스터로 취급하는게 바람직하다. 상태가 있는 서버 클러스터의 규모를 늘리고 줄이는 것은 운영 부담과 위험이 큰 작업이니 주의 깊게 계획하고 진행해야 함
- 혼잡 시간대 트래픽을 무리 없이 감당하고 불필요한 크기 변화를 피할 수 있도록 어느 정도 여유를 두고 오버 프로비저닝(over provisioning)하는 것이 좋다.
- 상태가 있는 클러스터 규모를 늘릴 때 발생할 수 있는 문제
  - 많은 채널이 같은 해시 링 위의 다른 여러 서버로 이동함, 서비스 탐색 컴포넌트가 모든 웹소켓 서버에 해시 링이 갱신되었음을 알리면 재구독 요청이 엄청나게 발생함
  - 재구독 요청을 처리하다 보면 클라이언트가 보내는 위치 정보 변경 메시지 처리가 누락될 수 있음, 그 빈도를 반드시 최소화해야 함
  - 서비스 상태가 불안정해질 수 있으므로 시스템 부하가 가장 낮은 시간대를 골라서 해야 함

- 클러스터의 크기를 조장하는 방법
  - 새로운 링 크기 계산
  - 해시 링의 키에 해당하는 값을 새로운 내용을 갱신
  - 대시보드를 모니터링함, 웹소켓 클러스터의 CPU 사용량이 어느 정도 튀는 것이 보여야 함


#### 친구 추가/삭제

- 친구가 추가되면 새로운 채널 구독이 필요하므로, 클라이언트는 연결된 웹소켓의 연결 핸들러에게 알려야 함
- 새 친구가 추가될 경우, 콜백을 해당 앱에 등록할 수도 있다. 이 콜백은 호출되면 웹소켓 서버로 새 친구의 펍/섭 채널을 구독하라는 메시지를 보낸다. 이 메시지를 처리하고 웹소켓 서버는 해당 친구가 활성화 상태인 경우 가장 최근 위치 및 시간 정보를 응답으로 리턴한다.
- 친구가 삭제될 경우에도 콜백을 등록할 수 있다. 이 콜백은 호출되면 웹소켓 서버로 해당 친구의 펍/섭 채널 구독을 해지하라는 메시지를 보낸다. 


#### 친구가 많은 사용자

- 친구가 많은 사용자는 시스템 성능 문제 가능성이 있다.
- 여기서는 최대로 맺을 수 있는 친구의 수에 상한이 있다고 가정했다.
- 친구 관게는 양방향이며, 팔로워 모델처럼 단방향 관계는 논의에서 배제할 것이다. 
- 수천 명의 친구를 구독하는 데 필요한 펍/섭 구독 관계는 클러스터 내의 많은 웹소켓 서버에 분산되어 있을 것이다.
- 따라서 핫스팟 문제는 발생하지 않을 것이다.


#### 주변의 임의 사용자

- 요구사항에는 없지만, 위치 정보 공유에 동의한 주변 사용자를 무작위로 보여줄 수 있도록 해보자고 하면 어떻게 할까?
- 기존 설계안에 지오해시에 따라 구축된 펍/섭 채널 풀을 두는 것이다. 지오해시 격자를 나누고, 각 격자마다 채널을 하나씩 만들어 두면 된다.
- 해당 격자 내의 모든 사용자는 해당 격자에 할당된 채널을 구독한다.

1. 사용자 2의 위치가 변경되면 웹소켓 연결 핸들러는 해당 사용자의 지오해시 ID를 계산한 다음, 해당 지오해시 ID를 담당하는 채널에 새 위치를 전송한다.
2. 근방에 있는 사용자 가운데 해당 채널을 구독하고 있는 사용자는 사용자 2의 위치가 변경되었다는 메시지를 수신한다.

- 격자 경계 부군에 있는 사용자까지 잘 처리하려면 클라이언트는 사용자가 위치한 지오해시뿐 아니라 주변 지오해시 격자를 담당하는 채널도 구독한다. 


#### 레디스 펍/섭 외의 대안

- 라우팅 계층으로 활용한 레디스 펍/섭 대신 얼랭을 사용할 수 있다.
- 얼랭은 고도로 분산된 병렬 애플리케이션을 위해 고안된 프로그래밍 언어이자 런타임 환경이다. 얼랭의 강력함은 경량 프로세스에서 나온다.
- 얼랭 프로세스 생성 비용은 리눅스 프로세스 생성 비용에 비하면 저렴하다. 가장 작은 얼랭 프로세스는 300바이트의 메모리만 사용하며, 최신 서버를 사용하면 한 대로 수백만 프로세스를 실행할 수 있다.
- 즉, 아무 일도 안 하는 얼랭 프로세스는 CPU 자원을 전혀 소모하지 않는다. 이 설계안의 천만 명의 활성 사용자 각각을 얼랭 프로세스로 모델링 할 수 있으며, 그 비용도 아주 저렴하다.
- 서버를 분산하기도 쉽다. 운영 부담도 낮으며 프로덕션 운영에서 발생하는 이슈의 실시간 디버깅을 지원하는 도구도 갖추고 배포 도구도 강력하다.
- 웹소켓 서비스는 얼랭으로 구현하고, 레디스 펍/섭 클러스터는 아예 분산 얼랭 애플리케이션으로 대체가 가능하다.
- 얼랭/OTP는 구독 기능을 내장하고 있어 구현은 쉬울 것이다. 
